{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019= pd.read_csv(\"C:/Ironhack/Week9-FinalProject/traffichistory-files/2019_Accidentalidad.csv\",  sep=';', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to map old column names to new column names\n",
    "column_mapping = {\n",
    "    'num_expediente': 'incident_number',\n",
    "    'fecha': 'date',\n",
    "    'hora': 'time',\n",
    "    'localizacion': 'location',\n",
    "    'numero': 'street_number',\n",
    "    'cod_distrito': 'district',\n",
    "    'distrito': 'district_name',\n",
    "    'tipo_accidente': 'accident_type',\n",
    "    'estado_meteorologico': 'weather_condition',\n",
    "    'tipo_vehiculo': 'vehicle_type',\n",
    "    'tipo_persona': 'person_type',\n",
    "    'rango_edad': 'age_range',\n",
    "    'sexo': 'gender',\n",
    "    'cod_lesividad': 'injury_code',\n",
    "    'lesividad': 'injury_type',\n",
    "    'coordenada_x_utm': 'utm_x_coordinate',\n",
    "    'coordenada_y_utm': 'utm_y_coordinate',\n",
    "    'positiva_alcohol': 'positive_alcohol_test',\n",
    "    'positiva_droga': 'positive_drug_test',\n",
    "}\n",
    "\n",
    "# Rename the columns using the mapping dictionary\n",
    "accidents_2019.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Display the DataFrame with the new column names\n",
    "accidents_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert 'date' and 'time' columns to a datetime object\n",
    "accidents_2019['datetime'] = pd.to_datetime(accidents_2019['date'] + ' ' + accidents_2019['time'])\n",
    "\n",
    "# Format the 'datetime' column as required\n",
    "accidents_2019['datetime'] = accidents_2019['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Display the result\n",
    "accidents_2019[['datetime']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSLATING COLUMN CONTENT FROM SPANISH TO ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepl==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_translate = ['accident_type', 'vehicle_type', 'person_type','injury_type', 'age_range', 'gender']\n",
    "\n",
    "# Initialize an empty dictionary to store unique values for each column\n",
    "unique_values_dict_ES = {}\n",
    "\n",
    "for col in columns_to_translate:\n",
    "    # Get unique values for the current column\n",
    "    unique_values_ES = accidents_2019[col].unique()\n",
    "\n",
    "    # Store the unique values in the dictionary\n",
    "    unique_values_dict_ES[col] = unique_values_ES\n",
    "\n",
    "# Print or inspect the dictionary of unique values\n",
    "unique_values_dict_ES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "# connection to deepl\n",
    "secrets_file = open(\"secrets_deepl.txt\", \"r\")\n",
    "key = secrets_file.read().strip()\n",
    "\n",
    "translator = deepl.Translator(key)\n",
    "\n",
    "# Columns to translate\n",
    "columns_to_translate = ['accident_type', 'vehicle_type', 'person_type','injury_type', 'age_range', 'gender']\n",
    "\n",
    "# Initialize an empty dictionary to store unique values for each column\n",
    "unique_values_dict = {}\n",
    "\n",
    "# Iterate through the specified columns\n",
    "for col in columns_to_translate:\n",
    "    # Get unique values for the current column\n",
    "    unique_values = accidents_2019[col].unique()\n",
    "\n",
    "    # Translate each unique value\n",
    "    translated_values = []\n",
    "    for value in unique_values:\n",
    "        try:\n",
    "            # Translate each value individually\n",
    "            translated_text = translator.translate_text(value, source_lang=\"es\", target_lang=\"en-GB\").text\n",
    "            translated_values.append(translated_text)\n",
    "        except Exception as e:\n",
    "            # Handle translation errors\n",
    "            print(f\"Translation error for value '{value}': {str(e)}\")\n",
    "            translated_values.append(value)  # Keep the original value\n",
    "\n",
    "    # Store the translated values in the dictionary\n",
    "    unique_values_dict[col] = translated_values\n",
    "\n",
    "# Print or inspect the dictionary of translated unique values\n",
    "print(unique_values_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_dict_ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = {}\n",
    "for key in unique_values_dict_ES.keys():\n",
    "    combined_dict[key] = dict(zip(unique_values_dict_ES[key], unique_values_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the map function to replace values in the DataFrame\n",
    "for col, translation_subdict in combined_dict.items():\n",
    "    accidents_2019[col] = accidents_2019[col].map(translation_subdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = accidents_2019.isna().sum()\n",
    "nan_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['vehicle_type'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['vehicle_type'] = accidents_2019['vehicle_type'].fillna('Not specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = accidents_2019.isna().sum()\n",
    "nan_counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "injury code and injury type describe the same information.  A description of the corresponding injury type for every injury code can be found in the README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019.drop(columns=['injury_type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['positive_alcohol_test'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['positive_alcohol_test'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values in positive_alcohol_test column with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['positive_alcohol_test'] = accidents_2019['positive_alcohol_test'].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change S (si) to Y (yes)\n",
    "accidents_2019['positive_alcohol_test'] = accidents_2019['positive_alcohol_test'].apply(lambda x: x.replace(\"S\", \"Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most positive_drug_test cells are NaN, thus this column will be dropped\n",
    "accidents_2019.drop(columns=['positive_drug_test'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019.to_excel('accidents_2019.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = accidents_2019.isna().sum()\n",
    "nan_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_with_nan = accidents_2019[accidents_2019['estado_meteorológico'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accidents_2019['estado_meteorológico'] = (accidents_2019['estado_meteorológico'].replace(\"Se desconoce\", \"NaN\"))\n",
    "\n",
    "\n",
    "accidents_2019['estado_meteorológico'] = accidents_2019['estado_meteorológico'].replace(\"NaN\", np.nan)\n",
    "\n",
    "# correct one\n",
    "# accidents_2019['estado_meteorológico'] = accidents_2019['estado_meteorológico'].replace(\"Se desconoce\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weather data \n",
    "\n",
    "weather_2019_daily= pd.read_csv('madrid 2019-01-01 to 2019-12-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2019_daily.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2019_daily['preciptype'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2019_daily['preciptype'] = weather_2019_daily['preciptype'].fillna('no rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019['estado_meteorológico'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2019_daily['preciptype'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_days = weather_2019_daily[weather_2019_daily['preciptype'] == 'rain']\n",
    "no_rain_days = weather_2019_daily[weather_2019_daily['preciptype'] != 'rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'date' is the name of your datetime column\n",
    "weather_2019_daily['datetime'] = pd.to_datetime(weather_2019_daily['datetime'])\n",
    "weather_2019_daily['datetime'] = weather_2019_daily['datetime'].dt.strftime('%d/%m/%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_with_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2019_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_2019.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_dates = weather_2019_daily[weather_2019_daily['precip'] == 'precip']['datetime'].tolist()\n",
    "\n",
    "# Create a copy of the DataFrame to avoid modifying the original\n",
    "accidents_copy = accidents_2019.copy()\n",
    "\n",
    "# Create a mask for rows with NaN values in 'estado_meteorológico'\n",
    "nan_mask = accidents_copy['estado_meteorológico'].isna()\n",
    "\n",
    "# Use 'date' column to check if the date is in the list of 'rain_dates'\n",
    "rain_mask = accidents_copy['datetime'].isin(rain_dates)\n",
    "\n",
    "# Fill 'estado_meteorológico' with 'precip' for dates with rain, and 'no_precip' otherwise\n",
    "accidents_copy.loc[rain_mask & nan_mask, 'estado_meteorológico'] = 'precip'\n",
    "accidents_copy.loc[~rain_mask & nan_mask, 'estado_meteorológico'] = 'no_precip'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_copy['estado_meteorológico'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for 'no precip' values\n",
    "no_precip_mapping = {'Despejado': 'no precip', 'no_precip': 'no precip', 'Nublado': 'no precip'}\n",
    "precip_mapping = {'Lluvia débil': 'precip', 'LLuvia intensa': 'precip', 'Granizando': 'precip', 'Nevando': 'precip'}\n",
    "\n",
    "# Apply the mapping to create a new column 'precipitation_status'\n",
    "accidents_copy['precipitation_status'] = accidents_copy['estado_meteorológico'].map(no_precip_mapping)\n",
    "\n",
    "# Apply the second mapping to fill remaining nan values in 'precipitation_status'\n",
    "accidents_copy['precipitation_status'].fillna(accidents_copy['estado_meteorológico'].map(precip_mapping), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_copy.to_excel('accidents_2019_final.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a model to assign values to NaN values in injury_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_copy= pd.read_excel('accidents_2019_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask to identify rows where 'injury_code' is equal to the value_to_drop\n",
    "mask = (accidents_copy['injury_code'] == 77)\n",
    "\n",
    "# Use the boolean mask to drop rows\n",
    "accidents_copy = accidents_copy[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = accidents_copy[accidents_copy['injury_code'].isna()]\n",
    "non_nan =  accidents_copy[accidents_copy['injury_code'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting numerical categorical and target\n",
    "\n",
    "categorical = non_nan[['district', 'accident_type', 'vehicle_type', 'person_type', 'age_range', 'gender', 'positive_alcohol_test', 'precipitation_status']]\n",
    "y=non_nan['injury_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "district                  int64\n",
       "accident_type            object\n",
       "vehicle_type             object\n",
       "person_type              object\n",
       "age_range                object\n",
       "gender                   object\n",
       "positive_alcohol_test    object\n",
       "precipitation_status     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encode_columns=categorical.drop(columns=['district'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0    16609\n",
       "7.0      7113\n",
       "2.0      2157\n",
       "6.0      1551\n",
       "1.0      1322\n",
       "5.0       715\n",
       "3.0       539\n",
       "4.0        34\n",
       "Name: injury_code, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "injury_code_distribution = accidents_copy['injury_code'].value_counts()\n",
    "injury_code_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_encode_columns, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding non categorical variables using onehot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an instance of OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore').fit(X_train)\n",
    "\n",
    "# Encode categorical features for both the training and test datasets\n",
    "categoricals_train_encoded_array = encoder.transform(X_train).toarray().copy()\n",
    "categoricals_test_encoded_array = encoder.transform(X_test).toarray().copy()\n",
    "\n",
    "# Create DataFrames with the encoded data and set the original index\n",
    "categoricals_train_encoded = pd.DataFrame(categoricals_train_encoded_array, index=X_train.index)\n",
    "categoricals_test_encoded = pd.DataFrame(categoricals_test_encoded_array, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled_smote, y_train_resampled_smote = smote.fit_resample(categoricals_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train).reset_index(drop=False)\n",
    "\n",
    "train_set = pd.concat([categoricals_train_encoded, y_train], axis=1).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\usuario\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'injury_code' is the column you want to balance\n",
    "target_column = 'injury_code'\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train_balanced = train_set.drop(target_column, axis=1)\n",
    "y_train_balanced = train_set[target_column]\n",
    "\n",
    "# Define the sampling strategy to balance the classes\n",
    "sampling_strategy = {\n",
    "    1.0: 13300,  # Adjust the number based on your preference\n",
    "    2.0: 13300,\n",
    "    3.0: 13300,\n",
    "    4.0: 5000,\n",
    "    5.0: 13300,\n",
    "    6.0: 13300,\n",
    "    7.0: 13300,\n",
    "    14.0: 5000,\n",
    "}\n",
    "\n",
    "# Create a pipeline with SMOTE for oversampling and RandomUnderSampler for undersampling\n",
    "pipeline = Pipeline([\n",
    "    ('oversample', SMOTE(sampling_strategy='auto', random_state=42)),\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy='auto', random_state=42))\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the training set\n",
    "X_train_pipelines, y_train_pipelines = pipeline.fit_resample(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pipelines_final= X_train_pipelines.drop(columns= 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     13263\n",
       "2.0     13263\n",
       "3.0     13263\n",
       "4.0     13263\n",
       "5.0     13263\n",
       "6.0     13263\n",
       "7.0     13263\n",
       "14.0    13263\n",
       "Name: injury_code, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pipelines.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4903462050599201\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.15      0.13      0.14       263\n",
      "         2.0       0.18      0.07      0.10       419\n",
      "         3.0       0.06      0.42      0.11        91\n",
      "         4.0       0.01      0.33      0.01         9\n",
      "         5.0       0.02      0.05      0.03       144\n",
      "         6.0       0.11      0.12      0.11       310\n",
      "         7.0       0.29      0.07      0.12      1426\n",
      "        14.0       0.78      0.81      0.79      3346\n",
      "\n",
      "    accuracy                           0.49      6008\n",
      "   macro avg       0.20      0.25      0.18      6008\n",
      "weighted avg       0.53      0.49      0.49      6008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_pipelines_final, y_train_pipelines)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_LogReg = model.predict(categoricals_test_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_LogReg))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_LogReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5744007989347537\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.15      0.10      0.12       263\n",
      "         2.0       0.17      0.07      0.10       419\n",
      "         3.0       0.09      0.20      0.12        91\n",
      "         4.0       0.02      0.11      0.03         9\n",
      "         5.0       0.05      0.05      0.05       144\n",
      "         6.0       0.08      0.08      0.08       310\n",
      "         7.0       0.39      0.24      0.30      1426\n",
      "        14.0       0.74      0.90      0.81      3346\n",
      "\n",
      "    accuracy                           0.57      6008\n",
      "   macro avg       0.21      0.22      0.20      6008\n",
      "weighted avg       0.53      0.57      0.54      6008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train the model\n",
    "model_MLPClassifier = MLPClassifier(max_iter=1000, random_state=42)\n",
    "model_MLPClassifier.fit(X_train_pipelines_final, y_train_pipelines)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_MLPClassifier = model_MLPClassifier.predict(categoricals_test_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_MLPClassifier))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_MLPClassifier))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
